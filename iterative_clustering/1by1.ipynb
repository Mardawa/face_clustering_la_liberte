{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from iterative_clustering.iterative_clustering import clustering_1by1\n",
    "from face_clustering_pipeline import FaceClusteringPipeline\n",
    "from helper.compare_w_reference import compare_w_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this strategy, we will split the data (10-fold) and use 9 folds to create the\n",
    "\"baseline\" clusters and the remaining fold to be added 1 by 1 to the baseline\n",
    "clusters. \n",
    "\n",
    "We will then cluster all the outliers. If we consistently don't create any\n",
    "new clusters, this step could be skipped. \n",
    "\n",
    "Finally, we will compute the performance of this strategy (and compare it to our\n",
    "baseline strategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = pathlib.Path(\"/media/bao/t7/la_lib_dataset\")\n",
    "\n",
    "src_folder = base_path / \"img\"\n",
    "faces_folder = base_path / \"faces\"\n",
    "save_folder = base_path / \"results_iterative_clustering\"\n",
    "\n",
    "df_folder = save_folder / \"df_1by1\"\n",
    "log_folder = save_folder / \"log_1by1\"\n",
    "\n",
    "df_folder.mkdir(exist_ok=True, parents=True)\n",
    "log_folder.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the (pre-computed) embeddings \n",
    "model_name = \"Facenet512\"\n",
    "df = pd.read_csv('/media/bao/t7/la_lib_dataset/results_dbscan/df/keep_representation_Facenet512.csv', converters={f\"{model_name}_representation\": json.loads}, usecols=[\"image\", f\"{model_name}_representation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceClusteringPipeline = FaceClusteringPipeline(src_folder, faces_folder, df_folder, log_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_clusters_path = pathlib.Path(\"../reference_clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [1:11:06<00:00, 1422.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# random_state is set for reproducibility\n",
    "for n_splits in tqdm([10, 50, 100]):\n",
    "    kf = KFold(n_splits=n_splits, random_state=42, shuffle=True) # we shuffle the data to avoid bias\n",
    "\n",
    "    for cluster_outlier in [\"all\", \"new\", \"skip\"]:\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1s = []\n",
    "        times = []\n",
    "\n",
    "        for idx, (index_baseline, index_1by1) in enumerate(kf.split(df)):\n",
    "\n",
    "            # if idx == 0:\n",
    "            #     print(f\"Length of the baseline: {len(index_baseline)}\")\n",
    "            #     print(f\"Length of the 1by1: {len(index_1by1)}\")\n",
    "\n",
    "            if idx >= 10:\n",
    "                break\n",
    "\n",
    "            # print(f\"Fold {idx}\")\n",
    "            df_baseline: pd.DataFrame = df.iloc[index_baseline].copy()\n",
    "            df_1by1: pd.DataFrame = df.iloc[index_1by1].copy()\n",
    "\n",
    "            # print len\n",
    "            # print(f\"df_baseline: {len(df_baseline)}\")\n",
    "            # print(f\"df_1by1: {len(df_1by1)}\")\n",
    "\n",
    "            # t is the time for 1by1 clustering, comparison with the base cluster only \n",
    "            # assumes that the base cluster is already computed\n",
    "            # don't include the outlier clustering time\n",
    "            res = clustering_1by1(df_baseline, df_1by1, cluster_outlier=cluster_outlier, faceClusteringPipeline=faceClusteringPipeline, \n",
    "            df_folder=df_folder,\n",
    "            model_name=model_name)\n",
    "            df_res = res[0]\n",
    "            t = res[1]\n",
    "\n",
    "            times.append(t)\n",
    "\n",
    "            total_tp, total_fn, total_fp, df_stats = compare_w_ref(reference_clusters_path, df_res, faces_folder=faces_folder, src_folder=src_folder)\n",
    "\n",
    "            total_precision = total_tp / (total_tp + total_fp)\n",
    "            precisions.append(total_precision)\n",
    "\n",
    "            total_recall = total_tp / (total_tp + total_fn)\n",
    "            recalls.append(total_recall)\n",
    "\n",
    "            total_f1 = 2 * (total_precision * total_recall) / (total_precision + total_recall)\n",
    "            f1s.append(total_f1)\n",
    "\n",
    "        summary = pd.DataFrame({\"precision\": precisions, \"recall\": recalls, \"f1\": f1s, \"time\": times})\n",
    "\n",
    "        # save the summary\n",
    "        summary.to_csv(f\"res/res_1by1_{n_splits}_{cluster_outlier}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('snowflakes')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01f49e0e7f49e8c39b51c5bad5f7f78046f883f46ef3b19f9dfb3452a1233494"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
