{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import perf_counter\n",
    "from sklearn.model_selection import KFold \n",
    "from ast import literal_eval\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from face_clustering_pipeline import FaceClusteringPipeline\n",
    "from iterative_clustering.iterative_clustering import split_clustering, merge_clustering\n",
    "from helper.compare_w_reference import compare_w_ref\n",
    "from helper.display_cluster import faceId_to_ogId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Divide the full dataset into <n> subsets\n",
    "* Cluster each subset using DBSCAN (best f1 parameter)\n",
    "* Compute the average embedding for each predicted cluster\n",
    "* Merge the clusters from each subset into a single set of clusters (using the\n",
    "  average embedding) => should try using either DBSCAN or AHC (average linkage)\n",
    "* Compute performance metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = pathlib.Path(\"/media/bao/t7/la_lib_dataset\")\n",
    "\n",
    "src_folder = base_path / \"img\"\n",
    "faces_folder = base_path / \"faces\"\n",
    "save_folder = base_path / \"results_iterative_clustering\"\n",
    "\n",
    "df_folder = save_folder / \"df_divide_merge\"\n",
    "log_folder = save_folder / \"log_divide_merge\"\n",
    "\n",
    "df_folder.mkdir(exist_ok=True, parents=True)\n",
    "log_folder.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceClusteringPipeline = FaceClusteringPipeline(src_folder, faces_folder, df_folder, log_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the (pre-computed) embeddings \n",
    "model_name = \"Facenet512\"\n",
    "df = pd.read_csv(base_path / 'results_dbscan/df/keep_representation_Facenet512.csv', converters={f\"{model_name}_representation\": json.loads}, usecols=[\"image\", f\"{model_name}_representation\"])\n",
    "\n",
    "df[\"face_id\"] = df[\"image\"].apply(pathlib.Path).apply(lambda x: x.stem)\n",
    "df[\"id\"] = df[\"face_id\"].apply(faceId_to_ogId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the metadata\n",
    "\n",
    "df_metadata = pd.DataFrame()\n",
    "\n",
    "for idx in range(1, 5):\n",
    "    metadata = pd.read_csv(f\"/media/bao/t7/la_lib_dataset/df_w_metadata/df{idx}.csv\", converters={\"metadata\": literal_eval})\n",
    "    df_metadata = pd.concat([df_metadata, metadata], ignore_index=True)\n",
    "\n",
    "df_metadata['assetCreated'] = df_metadata['metadata'].apply(lambda x: x.get('assetCreated').get('value'))\n",
    "\n",
    "# left merge df and df_metadata\n",
    "df = df.merge(df_metadata[['id', 'assetCreated']], on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['assetCreated'].isnull().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [08:56<00:00, 59.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# sort df by assetCreated\n",
    "df = df.sort_values(by=\"assetCreated\")\n",
    "\n",
    "for n_splits in tqdm(range(2, 11)):\n",
    "    for cluster_outlier in [\"all\", \"skip\"]: # [\"all\", \"skip\"]\n",
    "            precisions = []\n",
    "            recalls = []\n",
    "            f1s = []\n",
    "            times = []\n",
    "\n",
    "            df_splits = np.array_split(df, n_splits)\n",
    "            \n",
    "            total_t = 0\n",
    "            df_res, t_res = split_clustering(df_splits, faceClusteringPipeline=faceClusteringPipeline, df_folder=df_folder, model_name=model_name)\n",
    "            total_t = sum(t_res)\n",
    "            \n",
    "            if cluster_outlier == \"all\":\n",
    "                t0 = perf_counter()\n",
    "\n",
    "                df_inliers = df_res[df_res[\"cluster_label\"] != -1]\n",
    "                df_outliers = df_res[df_res[\"cluster_label\"] == -1][[\"image\", f\"{model_name}_representation\"]]\n",
    "\n",
    "                df_outliers_res, _ = split_clustering([df_outliers], faceClusteringPipeline=faceClusteringPipeline, df_folder=df_folder, model_name=model_name)\n",
    "                clustering_res = [df_inliers, df_outliers_res]\n",
    "                df_res = merge_clustering(clustering_res, faceClusteringPipeline=faceClusteringPipeline, df_folder=df_folder, model_name=model_name)\n",
    "                t1 = perf_counter()\n",
    "\n",
    "                total_t += t1-t0\n",
    "\n",
    "            df_res[\"face_id\"] = df_res[\"image\"].apply(pathlib.Path).apply(lambda x: x.stem)\n",
    "\n",
    "            reference_clusters_path = pathlib.Path(\"../reference_clusters\")\n",
    "\n",
    "            total_tp, total_fn, total_fp, df_stats = compare_w_ref(reference_clusters_path, df_res, faces_folder=faces_folder, src_folder=src_folder)\n",
    "\n",
    "            total_precision = total_tp / (total_tp + total_fp)\n",
    "            total_recall = total_tp / (total_tp + total_fn)\n",
    "            total_f1 = 2 * (total_precision * total_recall) / (total_precision + total_recall)\n",
    "\n",
    "            precisions.append(total_precision)\n",
    "            recalls.append(total_recall)\n",
    "            f1s.append(total_f1)\n",
    "            times.append(total_t)\n",
    "\n",
    "            df_results = pd.DataFrame({\n",
    "                \"precision\": precisions,\n",
    "                \"recall\": recalls,\n",
    "                \"f1\": f1s,\n",
    "                \"time\": times\n",
    "            })\n",
    "\n",
    "            df_results.to_csv(f\"res/res_dmdate_{n_splits}_{cluster_outlier}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('snowflakes')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01f49e0e7f49e8c39b51c5bad5f7f78046f883f46ef3b19f9dfb3452a1233494"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
